syntax = "proto2";

package proto.bolt;

import "neuron_index.proto";
import "optimizers.proto";
import "parameter.proto";

enum ActivationFunction {
  RELU = 0;
  SOFTMAX = 1;
  SIGMOID = 2;
  TANH = 3;
  LINEAR = 4;
}

message FullyConnected {
  required uint64 dim = 1;
  required uint64 input_dim = 2;
  required float sparsity = 3;
  required ActivationFunction activation = 4;

  required bool use_bias = 5;

  required Parameter weights = 6;
  required Parameter bias = 7;

  required uint64 rebuild_hash_tables = 8;
  required uint64 reconstruct_hash_functions = 9;

  optional NeuronIndex neuron_index = 10;
  required bool index_frozen = 11;

  optional Optimizer weight_optimizer = 12;
  optional Optimizer bias_optimizer = 13;
  required bool disable_sparse_parameter_updates = 14;
}

message Embedding {
  required uint64 dim = 1;
  required uint64 input_dim = 2;
  required ActivationFunction activation = 3;

  required bool use_bias = 4;

  required Parameter embeddings = 5;
  required Parameter bias = 6;

  optional Optimizer embeddings_optimizer = 7;
  optional Optimizer bias_optimizer = 8;
  required bool disable_sparse_parameter_updates = 9;
}

enum EmbeddingReduction {
  CONCAT = 0;
  SUM = 2;
  AVG = 3;
}

message RobeZ {
  required uint64 num_lookups_per_token = 1;
  required uint64 lookup_size = 2;
  required uint64 log_embedding_block_size = 3;
  required EmbeddingReduction reduction = 4;

  optional uint64 num_tokens_per_input = 5;
  required uint64 update_chunk_size = 6;
  required uint32 hash_seed = 7;

  required Parameter embedding_block = 8;

  optional Optimizer embedding_block_optimizer = 9;
  required bool disable_sparse_parameter_updates = 10;
}

message LayerNorm {
  required Parameter gamma = 1;
  required Parameter beta = 2;

  optional Optimizer gamma_optimizer = 3;
  optional Optimizer beta_optimizer = 4;
}

message Concatenate {}

message Tanh {}

message Op {
  required string name = 1;

  oneof type {
    FullyConnected fully_connected = 2;
    Embedding embedding = 3;
    RobeZ robez = 4;
    LayerNorm layer_norm = 5;
    Concatenate concatenate = 6;
    Tanh tanh = 7;
  }
}