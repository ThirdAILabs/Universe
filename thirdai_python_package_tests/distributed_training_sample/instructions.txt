For running the distributed bolt, first we need to start a ray cluster
For starting the ray cluster:
        - Run the command: python3 start_cluster.py <head_node_ip> <worker_node_ip_1> <worker_node_ip_2> ...


To check whether cluster have started run command 'ray status'
        - If the number of cpus id equal to (no. of ips)* 48 => cluster is setup

Once cluster is started to start training
        Importing the library
        - from thirdai.distributed_bolt import DistributedBolt


        The current APIs supported:

        -        head = DistributedBolt(1 /** number of workers you want to use, should be less than or equal to the number of ips in the cluster**/, config_filename/** configuration file **/) 

        -        head.train(False/** True=> circular communication or False=> Linear Communication**/) Prefer Linear Communication for usage upto 3 nodes.
        
        -        print(head.predict()) /**Return is same as network.predict()**/

Look at train_distributed_amzn670k.py for sample code.

IMPORTANT
1. head_node_ip should be the same ip of machine the script is running
2. Make sure you have the Distributed_Bolt_V1 branch build on everynode you are running.