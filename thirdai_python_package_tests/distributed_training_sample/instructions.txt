For running the distributed bolt, first we need to start a ray cluster

Below Cluster Initialization would only work for Custom Clusters, for standard clusters like AWS, GCP, Azure and Aliyun, see: https://docs.ray.io/en/latest/cluster/cloud.html.


First install all the required module by running the following command(to be done on all the node):
        - Have the Distributed bolt module built on those node 
        - Run the command: pip install -r requirements.txt
        - Check if ray path is included in system path by printing (run 'echo $PATH' on terminal)
                Ray's default path is /home/$USER/.local/bin
                If not included, run the command: export PATH=$PATH:/home/$USER/.local/bin
        
Automatic Cluster Initialization:
        
        Fill in the cluster configuration YAML file(cluster_configuration.yaml in cluster_configuration_files): 
        Edit all the required field in the cluster_configuration.yaml
        head_ip: Add the IP for head node 
        workers_ip: Add the IP for the workers node in the list
        ssh_private_key: Uncomment if ssh passwordless loging is not there on the nodes
        main_workers, max_workers: By default make them min_workers == max_workers == len(worker_ips)

        For starting the ray cluster automatically:
                - Run the command: python3 start_cluster.py cluster_configuration.yaml
        
        For stopping the ray cluster automatically:
                - Run the coammand: python3 stop_cluster.py cluster_configuration.yaml
                
                
Manual Cluster Initialization:
        For starting ray cluster manually:
                - On head node, run the command 'ray start --head --port=6379'
                - On worker nodes, run the command 'ray start --address=<head_node_ip>:6379'
               


To check whether cluster have started run command on terminal 'ray status'
        - See Node Status

Once cluster is started to start training
        Importing the library
        - from thirdai.distributed_bolt import DistributedBolt


        The current APIs supported:

        -        head = DistributedBolt(1 /** number of workers you want to use, should be less than or equal to the number of ips in the cluster**/, config_filename/** configuration file **/) 

        -        head.train(False/** True=> circular communication or False=> Linear Communication**/, num_cpus_per_node=k(set number of cpus here manually)) 
        
        -        print(head.predict()) /**Return is same as network.predict()**/

Look at train_distributed_amzn670k.py for sample code.

IMPORTANT
1. head_node_ip should be the first input of ray cluster
2. Make sure you have the DistributedBolt_V1 branch is built on everynode(head node and worker node) you are running.
3. Set up password less ssh between nodes(for easier usage)
4. If the num_cpus_per_node is not set, DistributedBolt will automatically get the number of cpus available on the 
   current and initialize the worker node with that count.