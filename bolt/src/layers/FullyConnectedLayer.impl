namespace thirdai::bolt {

template <bool DENSE, bool PREV_DENSE>
void FullyConnectedLayer::forwardImpl(const BoltVector& input,
                                      BoltVector& output,
                                      const BoltVector* labels) {
  assert((input.len <= _prev_dim && !PREV_DENSE) ||
         (input.len == _prev_dim && PREV_DENSE));
  assert((input.active_neurons == nullptr && PREV_DENSE) ||
         (input.active_neurons != nullptr && !PREV_DENSE));
  assert((output.len <= _dim && !DENSE) || (output.len == _dim && DENSE));
  assert((output.active_neurons == nullptr && DENSE) ||
         (output.active_neurons != nullptr && !DENSE));
  assert(labels == nullptr || labels->len > 0);

  selectActiveNeurons<DENSE, PREV_DENSE>(input, output, labels);

  float max_act = 0;
  uint32_t len_out = DENSE ? _dim : _sparse_dim;
  std::fill_n(output.gradients, len_out, 0);

  _prev_is_dense = PREV_DENSE;
  _this_is_dense = DENSE;

  if (!DENSE && !PREV_DENSE) {
    std::unique_ptr<ActiveNeuronsPair> active_pairs =
        std::make_unique<ActiveNeuronsPair>(std::vector<uint64_t>(),
                                            std::vector<uint64_t>());
    for (uint64_t i = 0; i < input.len; i++) {
      active_pairs->first.push_back(input.active_neurons[i]);
    }
    for (uint64_t n = 0; n < len_out; n++) {
      active_pairs->second.push_back(output.active_neurons[n]);
    }
#pragma omp critical
    _active_pairs.push_back(std::move(active_pairs));
  }

  if (!DENSE) {
    for (uint64_t n = 0; n < len_out; n++) {
      uint64_t act_neuron = output.active_neurons[n];
      _is_active[act_neuron] = true;
    }
  }

  if (!PREV_DENSE) {
    for (uint64_t i = 0; i < input.len; i++) {
      uint64_t act_neuron = input.active_neurons[i];
      _prev_is_active[act_neuron] = true;
    }
  }

  for (uint64_t n = 0; n < len_out; n++) {
    // Because DENSE is known at compile time the compiler can remove this
    // conditional
    uint64_t act_neuron = DENSE ? n : output.active_neurons[n];
    assert(act_neuron < _dim);
    float act = _biases[act_neuron];
    for (uint64_t i = 0; i < input.len; i++) {
      // Because PREV_DENSE is known at compile time the compiler can remove
      // this conditional
      uint32_t prev_act_neuron = PREV_DENSE ? i : input.active_neurons[i];
      assert(prev_act_neuron < _prev_dim);
      act += _weights[act_neuron * _prev_dim + prev_act_neuron] *
             input.activations[i];
    }

    assert(!std::isnan(act));

    switch (_act_func) {
      case ActivationFunction::ReLU:
        if (act < 0) {
          output.activations[n] = 0;
        } else {
          output.activations[n] = act;
        }
        break;
      case ActivationFunction::Softmax:
        output.activations[n] = act;
        if (max_act < act) {
          max_act = act;
        }
        break;
      case ActivationFunction::Sigmoid:
        output.activations[n] = 1 / (1 + std::exp(-act));
        break;
      case ActivationFunction::Linear:
        output.activations[n] = act;
        break;
      case ActivationFunction::Tanh:
        output.activations[n] = static_cast<float>(std::tanh(act));
        break;
    }
  }

  if (_act_func == ActivationFunction::Softmax) {
    float total = 0;
    for (uint64_t n = 0; n < len_out; n++) {
      output.activations[n] = std::exp(output.activations[n] - max_act);
      total += output.activations[n];
    }
    for (uint64_t n = 0; n < len_out; n++) {
      output.activations[n] /= (total + EPS);
      assert(!std::isnan(output.activations[n]));
    }
  }
};

template <bool FIRST_LAYER, bool DENSE, bool PREV_DENSE>
void FullyConnectedLayer::backpropagateImpl(BoltVector& input,
                                            BoltVector& output) {
  assert((input.len <= _prev_dim && !PREV_DENSE) ||
         (input.len == _prev_dim && PREV_DENSE));
  assert((input.active_neurons == nullptr && PREV_DENSE) ||
         (input.active_neurons != nullptr && !PREV_DENSE));
  assert((output.len <= _dim && !DENSE) || (output.len == _dim && DENSE));
  assert((output.active_neurons == nullptr && DENSE) ||
         (output.active_neurons != nullptr && !DENSE));

  uint32_t len_out = DENSE ? _dim : _sparse_dim;

  for (uint64_t n = 0; n < len_out; n++) {
    assert(!std::isnan(output.gradients[n]));
    output.gradients[n] *= actFuncDerivative(output.activations[n], _act_func);
    assert(!std::isnan(output.gradients[n]));
    // Because DENSE is known at compile time the compiler can remove this
    // conditional
    uint32_t act_neuron = DENSE ? n : output.active_neurons[n];
    assert(act_neuron < _dim);
    for (uint64_t i = 0; i < input.len; i++) {
      // Because PREV_DENSE is known at compile time the compiler can remove
      // this conditional
      uint32_t prev_act_neuron = PREV_DENSE ? i : input.active_neurons[i];
      assert(prev_act_neuron < _prev_dim);
      _w_gradient[act_neuron * _prev_dim + prev_act_neuron] +=
          output.gradients[n] * input.activations[i];
      if (!FIRST_LAYER) {
        input.gradients[i] +=
            output.gradients[n] *
            _weights[act_neuron * _prev_dim + prev_act_neuron];
      }
    }
    _b_gradient[act_neuron] += output.gradients[n];
  }
}

template <bool DENSE, bool PREV_DENSE>
void FullyConnectedLayer::selectActiveNeurons(const BoltVector& input,
                                              BoltVector& output,
                                              const BoltVector* labels) {
  if (DENSE) {
    return;
  }

  std::unordered_set<uint32_t> active_set;

  uint32_t label_len = labels != nullptr ? labels->len : 0;
  for (uint32_t i = 0; i < label_len; i++) {
    assert(labels->active_neurons[i] < _dim);
    active_set.insert(labels->active_neurons[i]);
  }

  std::vector<uint32_t> hashes(_hasher->numTables());
  if (PREV_DENSE) {
    _hasher->hashSingleDense(input.activations, input.len, hashes.data());
  } else {
    _hasher->hashSingleSparse(input.active_neurons, input.activations,
                              input.len, hashes.data());
  }

  if (_sampling_mode == LSHSamplingMode::FreezeHashTablesWithInsertions) {
    /**
     * QueryBySet just returns a set of the elements in the given buckets of the
     * hash table.
     *
     * QueryAndInsertForInference returns the set of elements in the given
     * buckets but will also insert the labels (during training only) for the
     * vector into the buckets the vector maps to if they are not already
     * present in the buckets. The intuition is that during sparse inference
     * this will help force the hash tables to map vectors towards buckets that
     * contain their correct labels. This is specific to the output layer.
     *
     * We call QueryAndInsertForInference if the following conditions are met:
     *   1. We have sparse inference enabled.
     *   2. Activation = Softmax or Sigmoid, meaning it's a classification task,
     *      and that the given layer is the last layer, as this is the only
     *      place where we use these activation functions.
     */
    _hash_table->queryAndInsertForInference(hashes.data(), active_set,
                                            _sparse_dim);
  } else {
    _hash_table->queryBySet(hashes.data(), active_set);
  }
  if (active_set.size() < _sparse_dim) {
    // here we use hashes[0] as our random number because rand() is not thread
    // safe and we want to have deterministic outcomes
    uint32_t rand_offset = (hashes[0]) % _dim;
    while (active_set.size() < _sparse_dim) {
      active_set.insert(_rand_neurons[rand_offset++]);
      rand_offset = rand_offset % _dim;
    }
  }

  uint32_t cnt = 0;
  for (uint32_t i = 0; i < label_len; i++) {
    if (cnt == _sparse_dim) {
      break;
    }
    output.active_neurons[cnt++] = labels->active_neurons[i];
    active_set.erase(labels->active_neurons[i]);
  }

  for (auto x : active_set) {
    if (cnt == _sparse_dim) {
      break;
    }
    assert(x < _dim);
    output.active_neurons[cnt++] = x;
  }
}

}  // namespace thirdai::bolt
