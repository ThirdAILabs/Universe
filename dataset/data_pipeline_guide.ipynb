{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DataPipeline Guide**\n",
    "\n",
    "This notebook is a guide for DataPipeline, a configurable streaming dataset loader. DataPipeline's core functionality is to convert each row of a tabular dataset into a vector that Bolt can understand. What makes DataPipeline unique is that it allows you to configure what features to extract from a dataset and how to encode them as vectors. We do this by using what we call \"blocks\", objects that read specific columns of each row in a dataset and adds a feature segment to the row's corresponding vector.\n",
    "\n",
    "To illustrate, let's take a look at an example use case. First, we generate a mock dataset. Each row of this dataset has two comma-delimited columns. The first column contains a class ID and the second column contains a sentence. This is typical of a text classification dataset, where the task is to predict the class ID from the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 3\n",
    "n_samples = 10_000\n",
    "\n",
    "with open(\"test.csv\", \"w\") as f:\n",
    "    for i in range(n_samples):\n",
    "        sentiment = i % 3\n",
    "        if sentiment == 0:\n",
    "            f.write(\"0,bad stuff\\n\")\n",
    "        elif sentiment == 1:\n",
    "            f.write(\"1,good stuff\\n\")\n",
    "        else:\n",
    "            f.write(\"2,neutral stuff\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the shape of our dataset and the task that it is used for, we want to pass vectors that represent text from the second column to our Bolt model as input and pass vectors that represent categorical information from the first column as labels. This is what that looks like with DataPipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai.dataset import DataPipeline, blocks\n",
    "\n",
    "pipeline = DataPipeline(\n",
    "    filename=\"test.csv\", \n",
    "    input_blocks=[blocks.TextUniGram(col=1, dim=100_000)],\n",
    "    label_blocks=[blocks.NumericalId(col=0, n_classes=n_classes)],\n",
    "    batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've covered the main ideas, so now let's have a closer look at how we can configure this more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai.dataset import text_encodings, categorical_encodings\n",
    "\n",
    "pipeline = DataPipeline(\n",
    "    filename=\"test.csv\", \n",
    "    input_blocks=[\n",
    "        blocks.TextUniGram(col=1, dim=100_000),\n",
    "        blocks.TextPairGram(col=1, dim=100_000),\n",
    "        blocks.TextCharKGram(col=1, k=3, dim=100_000), # Character trigrams\n",
    "        blocks.NumericalId(col=0, n_classes=2), # Default encoding for categorical block\n",
    "    ],\n",
    "    label_blocks=[blocks.NumericalId(col=0, n_classes=n_classes)],\n",
    "    batch_size=256,\n",
    "    has_header=False, # Pipeline discards the header if the dataset has one.\n",
    "    delimiter=',') # Any character is a valid delimiter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Use case**\n",
    "As shown above, DataPipeline can be quite flexible â€“ in fact, too flexible to be given to customers. However, this can be useful in cases where we don't know how to best encode features for a new dataset or task. We can try different combinations of features without writing new data processing scripts, without recompiling C++ code after each iteration, and with the speed of a data-parallel C++ implementation. We can also use this tool to prototype new end-to-end models. For example, suppose I want to create an end-to-end sequential recommendation system. Instead of building a data loader from scratch, I can wrap DataPipeline in an \"autotuner\" of sorts. Of course, a hard-coded specialized system can be faster than a generic one, so we can then profile this prototype and hard-code the system to speed it up as needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Limitations**\n",
    "You may also have noticed that we don't have that many blocks at all! This is because we have only implemented the bare minimum for a specific use case. The good news is that the block interface is highly extendable and we can keep adding new blocks as we see fit. To do this, simply add a new implementation of the `Block` abstract class found in `dataset/src/blocks/BlockInterface.h`, then write python bindings and tests as necessary. You can refer to `dataset/src/blocks/Text.h` as an example `Block` implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Using DataPipeline with Bolt**\n",
    "Finally, we'll look at how we can pass the data to Bolt. While DataPipeline supports streaming, the Bolt Python API currently does not support it, so we will load the entire dataset into memory for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai import bolt\n",
    "\n",
    "pipeline = DataPipeline(\n",
    "    filename=\"test.csv\", \n",
    "    input_blocks=[blocks.Text(col=1, dim=100_000)],\n",
    "    label_blocks=[blocks.NumericalId(col=0, n_classes=n_classes)],\n",
    "    batch_size=256)\n",
    "\n",
    "# Load into memory and get input as well as label dimensions.\n",
    "data, labels = pipeline.load_in_memory()\n",
    "input_dim = pipeline.get_input_dim()\n",
    "n_classes = pipeline.get_label_dim()\n",
    "\n",
    "layers = [\n",
    "    bolt.FullyConnected(\n",
    "        dim=1000,\n",
    "        sparsity=0.1,\n",
    "        activation_function=bolt.ActivationFunctions.ReLU,\n",
    "    ),\n",
    "    bolt.FullyConnected(\n",
    "        dim=n_classes, activation_function=bolt.ActivationFunctions.Softmax\n",
    "    ),\n",
    "]\n",
    "\n",
    "network = bolt.Network(layers=layers, input_dim=input_dim)\n",
    "\n",
    "batch_size = 256\n",
    "learning_rate = 0.001\n",
    "epochs = 1\n",
    "for i in range(epochs):\n",
    "    network.train(\n",
    "        train_data=data,\n",
    "        train_labels=labels,\n",
    "        batch_size=batch_size,\n",
    "        loss_fn=bolt.CategoricalCrossEntropyLoss(),\n",
    "        learning_rate=learning_rate,\n",
    "        epochs=1,\n",
    "        verbose=True,\n",
    "    )\n",
    "    metrics, preds = network.predict(\n",
    "        test_data=data,\n",
    "        test_labels=labels,\n",
    "        batch_size=batch_size,\n",
    "        metrics=[\"categorical_accuracy\"],\n",
    "        verbose=True,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
